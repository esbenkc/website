---
created: 2024-10-11T03:32:11.792Z
title: Cybermorphism
slug: cybermorphism
date: 2024-10-14
categories: []
---
> *"A graphic representation of data abstracted from the banks of every computer in the human system. Unthinkable complexity. Lines of light ranged in the nonspace of the mind, clusters and constellations of data."* ([Neuromancer](https://www.google.com.sg/books/edition/Neuromancer/vi2KCwAAQBAJ?hl=en&gbpv=1&printsec=frontcover))

Imagine a future where the boundaries between human and machine blur - a world where our minds extend beyond our biological limits, seamlessly integrating with technology. This isn’t science fiction; it’s happening now.

I've lived most of my life experimenting with this human-machine interface, taking me from game development to brain-computer interfaces and cyborgism. In this post, I'll share the philosophical project, how we expand our cognition and how the 21st century will change consciousness.

Let's dive in.

![[Pasted image 20241014002141.png]]
*The [XREAL AR glasses](https://us.shop.xreal.com/products/xreal-air-2-pro) running through my [Jelly Star](https://kran.ai/jelly) with the [Beam stabilizer](https://us.shop.xreal.com/products/xreal-beam)[^1]*

[^1]: While writing this post fully in these same glasses, I notice the Beam Pro seems to have come out along with another iteration of the XREAL Air 2 Ultra glasses. I think my bank account will cry soon.
## Cybermorphism

The concept of cybermorphism is inspired by the extended mind thesis, _Neuromancer_ and "tools for thought". It's about expanding cognition, reaching beyond our 20th century cognition and creating a tech-enabled existence.

Cognition is always a moving goalpost and it changes much more than we care to think. Twenty years ago, no one used social media and now we check out phones [159](https://explodingtopics.com/blog/social-media-usage) times per day, changing how attention and cognition work. Books weren't readily available before the [15th century](https://en.wikipedia.org/wiki/Printing_press) and even then, free speech only arose in the [19th century](https://www.freespeechhistory.com/2019/11/21/episode-34-the-age-of-reaction-the-fall-and-rise-of-free-speech-in-19th-century-europe/). 

![[Pasted image 20241014005709.png]]
*The NIRSport2 SoTA fNIRS neuroimaging headset ([timelapse](https://youtu.be/5rnJgt_Xu-Q))*

As a result, you might argue that the common person's cognition before the 19th century was both more limited by lack of diverse intellectual input (pre-training) and by lack of expression (rewarded behavior), leading to a [lack of original cognition](https://en.wikipedia.org/wiki/Newspeak).

Now, whenever I talk with people born with the internet, I'm surprised at how informed and expressive they are. I don't think the previous generations could have [matched up at the same age](https://www.sciencedirect.com/topics/psychology/flynn-effect#:~:text=Abstract,approximately%203%20points%20per%20decade.). 
Historically, technology has been the enabler of cognition and we're finding more and more ways to design [tools for thought](https://numinous.productions/) to augment our brains.

Hence, **cybermorphism does not expect this trend of cognition improvement to stop**.

A good example of technology embracing the [extended mind thesis](https://academic.oup.com/analysis/article-abstract/58/1/7/153111?redirectedFrom=fulltext&login=false) (the idea that our cognition extends beyond our brain and into our tools) is [Obsidian](https://obsidian.md/). Our brain seems to think in concepts connected by neural pathways and not in information hierarchies like documentation systems and Obsidian embraces this.

![[Pasted image 20241014014105.png]]
*Each dot on the left is a page in my second brain with two examples of page text on the right - pink links represent references to other pages*

Similarly, we find that cybermorphosis changes between generations. The same people who grew up with the internet now [don't know what files are](https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z). It's all `cmd + k`, Spotlight and apps. 

Hierarchical file systems are great at managing documents but it might not *actually* be the best way to *work with concepts*. [Alfred](https://www.alfredapp.com/) is a Spotlight replacement that gives you an instant search field to your complete file system along with any extensions you want. It is the ultimate in file system independence.

## Brain cycles / action
Something you might notice with the above examples is that in cybermorphism, we're quite concerned with **how many brain cycles we need to use for each cognitive act**.

Similar to the maxims of engineering, we improve cognition by 1) removing unnecessary steps, 2) simplifying or optimizing and 3) automating. Sometimes, others have automated it for us, helping a lot (read: *all software*). 

Replacing file systems removes unnecessary hierarchical information navigation (est. 5 seconds), second brain note-taking replaces the need for strictly organized note-taking systems (est. 3 minutes per note) and [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) automates completing a project (est. $$\infty$$). Similarly, my hardware setups are usually optimized for easy access across the 20+ programs I use on a daily basis.

![[XREAL_1.gif]]
*Live view of my XREAL AR virtual monitor interface*

Of course, improving cognition isn't just about optimizing a specific process. Most of it will be about *creating new cognitive abilities*.
## Creating more senses
Now that we know our brain is spread across our internal neural circuitry and our tools, we might ask ourselves *"can we implement new features for this brain?"* Of course!

The best example of such a feature is **the keyboard**. A [new limb](https://x.com/EsbenKC/status/1844620047182070262) for extended cognition that depends on our brain recoding our fingers' specialization.

For people with reduced bodily function (such as [paraplegics](https://www.sciencedirect.com/topics/medicine-and-dentistry/paraplegia) and [Stephen Hawking](https://www.washington.edu/doit/dr-stephen-hawking-case-study-using-technology-communicate-world#:~:text=By%20squeezing%20his%20cheek%20muscles,speak%22%20through%20a%20voice%20synthesizer.)), these new modalities of action (so-called "external effectors") enable them to live a semi-normal life. And while Hawking in the early 2010s used the few muscles he could still control in a clever way to communicate, today's Neuralink enables [Noland Arbaugh to pull Civ VI all-nighters](https://www.reddit.com/r/civ/comments/1ctgui8/noland_arbaugh_using_neuralink_to_play_civ6/). The progress here is insane.

![[Pasted image 20241014012236.png]]
*Me calibrating the EyeLink 1000, a 1,000 hz 0.15° accuracy eye tracker*

I still remember when we received a grant to purchase the (very expensive) [NIRSport2](https://nirx.net/nirsport) for our lab which was the state-of-the-art fNIRS neuroimaging equipment at the time. About a week after we received it, the revolutionary [Kernel Flow](https://www.kernel.com/products) came out, shaking up the field completely.

Of course, besides my endless 4am sessions in the lab with the amazing NIRSport2, I ended up playing with the beta Kernel Flow through a Canadian Lab I connected with in our NeuroTechX network. This is one of the reasons why I'm connected with humanity's most experimented upon man and original founder of Kernel, [Bryan Johnson](https://www.standard.co.uk/lifestyle/wellness/bryan-johnson-anti-aging-biological-age-son-longevity-b1139278.html), on LinkedIn.

## Aside: The science behind cognition

Without understanding the brain, we cannot understand how to develop and research cybermorphism. This is a very short primer on cognition for the initiated:

We know surprisingly little about how cognition works. With the technical baggage from the incumbent outside-in perspective on which functions the brain encodes (thanks, psychology), many theories still need evidence before we can verify them.

One of the best ways to think of the neocortex (our main tool in cybermorphism) is as a collection neural networks connected by wires in useful ways. 

Specifically, think of it as [52 distinct neural network architectures](https://en.wikipedia.org/wiki/Brodmann_area) (illustrative) designed through evolution for various functions, each with different inputs and outputs. Some receive audio input, some visual input and some receive input from other areas of the brain.

Each of these areas can be modeled separately as an "inside-out" process. Similar to the famous ["What is it like to be a bat?"](https://philpapers.org/go.pl?id=NAGWII&proxyId=&u=http%3A%2F%2Fwww.jstor.org%2Fstable%2Fpdfplus%2F2183914.pdf), you should now imagine what it is like to be that neural network.
- You receive signals of unknown origin in an unknown format that you are somewhat familiar with by design.
- If your output is useful for predicting the next second or minute (so-called ["predictive coding"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6632880/)), you get a reward.
- You begin to update your transformation of these inputs to get the best rewards.

Traditionally, cognitive psychology has tried to find the neural representations of abilities such as "reasoning", "will" and "emotions". These come from the 1890 book ["The Principles of Psychology"](https://psychclassics.yorku.ca/James/Principles/index.htm) which at the time was a revolutionary book. However, it is outdated now.

![[Pasted image 20241014011734.png]]
*"How much hardware do you want for your cybermorphosis?" "Yes." (the [NIRSport2](https://nirx.net/nirsport) after the 30 minute setup process, ready to capture brain signals)*

As you imagine what it is like to be a neural network in the brain, you will realize that you have no clue what "reasoning", "will" or "emotions" are and simply do what is optimal, e.g. model "time" as a sequence similar to how you would model "a series of location", meaning that these are not two different types of encodings.

For more context, I highly recommend reading the book ["The Brain From Inside Out"](https://academic.oup.com/book/35081). If you're already in the literature of the brain, ["The Continuity of Mind"](https://global.oup.com/academic/product/the-continuity-of-mind-9780195370782) should be of interest to shake up your thinking.

## Aside 2: The brain easily learns new abilities

One of the seminal papers on brain-machine interfacing was the [2000 paper by Chapin and others](https://www.nature.com/articles/nn0799_664). They got a rat to control a water dispenser simply through neural activation. With a neural network, they modeled which neurons activated when the rat pulled a lever, used that model to activate the water dispenser when these neurons fired and the rats ended up learning how to control the water dispenser just with their mind.

However, it turns out that we don't need the computer to do any coding at all. We can simply say "when this neuron fires, we pull the lever" and the brain will quickly learn how to activate that specific neuron at the correct time to receive the right feedback from its environment. Of course, implementing learning on both sides of that system helps the process along.

I highly recommend the [short paper from Neuralink](https://www.jmir.org/2019/10/e16194/) detailing how they record neural activity.

![[Pasted image 20241014005626.png]]
*My profile's YouTube cover image of 3 years is the MRI recording of my [T1 scan](https://radiopaedia.org/articles/t1-weighted-image) seen on my [About page](https://blog.kran.ai/about)*

## The future of cognition
If you're like me and ended up in brain-computer interfacing research as a result of cybermorphism, you probably also ended up working with software after realizing that brain-computer interfaces were too immature[^3]

[^3]: At an event in San Francisco I was made privy to the inside stories of funding for brain augmentation and it doesn't look good. There's no funding except for pharmaceutical interventions that have American insurance companies as customers. As a result, the only progress has been slow ([Blackrock Neurotech](https://blackrockneurotech.com/products/utah-array/#:~:text=What%20is%20the%20Utah%20Array,degree%20of%20precision%20and%20accuracy.) and [Nirx](https://nirx.net/)), self-funded (see [Musk](https://neuralink.com/) and [Johnson](https://kernel.co/)) or from [the military](https://www.from-the-interface.com/DARPA-funding-BCI-research/) (yuck).

Today, however, the hardware space is changing more than ever.
### Cybermorph hardware
For me, cybermorph hardware comes in many forms:
- Multi-channel invasive neural implants for direct neural interaction (such as Neuralink)
- Redesigned keyboards with new types of mapping to improve [WPM](https://en.wikipedia.org/wiki/Words_per_minute)
- Direct current stimulation to key areas of the brain using tDCS devices
- AR and VR glasses designed to improve how we interact with our extended mind
- New external effectors such as bionic limbs and prosthetics

Brain-computer interfaces are one of the most obvious future developments in this space. To **read brain activity and use it to control our world**, the main interfaces will probably be invasive hardware like the Neuralink, the Utah Array or ECoG. Having worked with non-invasive hardware like fNIRS, fMRI and EEG, they are simply too imprecise to be used for actual control. Micro-EEG devices like the [Ear-EEG](https://en.wikipedia.org/wiki/Ear-EEG) might be useful for concentration monitoring ([and maybe even more](https://academic.oup.com/book/11166)).

To transmit information directly to the brain, I similarly expect that direct neural interfaces will be the only viable path. [TMS](https://www.mayoclinic.org/tests-procedures/transcranial-magnetic-stimulation/about/pac-20384625) is currently used in clinical settings and has been shown to be useful in [memory recall tasks](https://www.sciencedirect.com/science/article/abs/pii/S0006899306029830) besides its current use against clinical depression. [tDCS](https://www.nature.com/articles/s41598-023-29792-6#:~:text=Transcranial%20direct%20current%20stimulation%20(tDCS)%20is%20an%20emerging%20neuromodulation%20technique,a%20targeted%20brain%20region3.) is also used in clinical settings but some interesting claims show their *very slight* stimulation impact might [boost creativity](https://www.platoscience.com/pages/science). Most applications of tDCS do not have any significant effect today, however.

Direct neural stimulation will take the same form as Neuralinks and we'll have to see how cybersecurity will solve the problem of directly modifiable brains. The technology is pretty straightforward - you just send a very tiny shock into the electrodes put into the brain which causes the "activation threshold" (when a neuron fires) to be reached for neurons in its vicinity. 

Right now, most reading and stimulation happens in the touch (writing) and limb movement (reading) parts of the brain because we have the best understanding of how these parts of the neocortex work and the limb movement part (the "motor cortex") is designed to control external hardware already (hands and arms).

We can probably expect this reading and writing to expand to other regions of the brain, such as the language understanding areas on the left side of our brain. In the longer-term, this should enable us to communicate complex concepts directly to and from a computer with neural activation.

It feels like the most progress in understanding cognition happens in brain-computer interface research because we get a direct signal if something works or doesn't work immediately. I'm personally looking forward to increasing our understanding of the front parts of our brain (usually proposed to relate to personality and higher-level planning) and the right side of our brain.

Might we see a full computer controllable through a complete neural interface? Will we be able to get more brain power with external compute? Can we create safe and reliable interfaces without risk of short-circuiting our brains? Will it one day be possible to create a complete human upload and live as a digital sentience?

![[Pasted image 20241014012320.png]]
*Sending direct current through my brain with the PlatoWork tDCS beta device from my friends over at [PlatoScience](https://www.platoscience.com/pages/product)*

On the other side of hardware, I personally use the XREAL Air 2 Pro glasses and there is an incredible immersion in a virtual monitor setup. I wrote my thesis completely in the Oculus Link interface on my Quest II and with developments such as the Apple Vision Pro, these seem to become more normal. With these kinds of devices miniaturized ([maybe even to the size of our pupils?](https://www.tandfonline.com/doi/full/10.1080/08164622.2023.2188176)), reality will probably continue to be augmented with even more information than they are today.

Gesture-based devices such as the [humane pin](https://humane.com/) have clearly been a flop until now, but with developments in personal assistants, these will probably become more common as reliability improves.

With projects such as [Blueprint](https://blueprint.bryanjohnson.com/?srsltid=AfmBOooG5JBooJc7gypSscc0tJTvMs3QFxASTQMOgCd-DNkdcfwH2Hv1) and the development of [accurate body sensory hardware](https://www.youtube.com/thequantifiedscientist), it will be interesting to see how our interfaces will respond and track our bodily states. Emotional modeling and stress response might become more commonplace to involve in our interfaces as these are currently not involved in our processes.

Another fascinating hardware question will be the development of robots. Users of [character.ai](https://beta.character.ai/chats) (a chatbot service where you can chat with personalities that remember your relationship) already seem to be spending [two hours a day](https://qz.com/a-startup-founded-by-former-google-employees-claims-tha-1850919360) chatting with fake beings. And if that trend continues, establishing personal relationships with robots and chatbots is not far off. One interesting problem will be the disconnect between the [chatbot's intelligence and the user's intelligence](https://x.com/EsbenKC/status/1831051342963769719) and how [robot rights](https://www.europarl.europa.eu/RegData/etudes/STUD/2016/563501/EPRS_STU%282016%29563501_EN.pdf) might evolve.

Will we see neural, voice or gesture based interfaces overtake typing and text? How will file systems change into something even closer to how our brains work (potentially inspired by [vector databases](https://en.wikipedia.org/wiki/Vector_database) and maybe even the [cyberspace](https://www.historyofinformation.com/detail.php?id=983))? Can the integration of personal assistants make manual action sequences meaningless? Might we see more human user experiences that help you process and learn about your emotions as you interact?
### Predictions: Cybermorph brain augmentation

 To make the above discussion more concrete, I'll share some of my personal expectations for this century:
 
- Within ten years, Neuralink will be available for healthy consumers, either privately or publicly (80%).
- Pharmaceutical brain augmentation (whether nanobots or drugs, excluding caffeine, nicotine and pseudoscience) will be commonplace and estimated at 1%< use within the working population by 2035 (90%).
- Integrated biometric tracking (e.g. glucose measurement devices, watches do not count) will be commonplace (1%< of the US population) by 2030 (80%).
- Consumer AR glasses will become commonplace (1% adoption) for public wear by 2030 (75%).
- The _consumer interface_ with machines, independent of platform, will _not_ be by text in 2035 (80%). Gesture or voice are the expected contenders but I won't leave direct-to-neural out of the race.
- Apple Vision Pro will be seen as a _financial_ flop (if it isn't already) in a year (95%).
- Neural brain augmentation using external hardware will be commonplace (1%< of the US population) by 2040 (80%).
- Sensory augmentation (e.g. magnetic field sensors, wider color spectrum and cyberspace connection) will be commonplace (1%< of US population) by 2040 (70%).
- Networked human-mind interfaces between multiple humans will be possible by 2060 (60%).
- A complete human brain upload with confirmation of the retention of memories and general cognitive ability (barring designed augmentation) will be completed before 2060 (40%).

### Cyborgism
[Cyborgism](https://cyborgism.wiki/) is a version of cybermorphism whose practitioners dedicate their minds and brains to interfacing with AI. It's the group that consistently has the most understanding of what AIs are capable of and the group that develops the most advanced interfaces into large language models (LLMs).

This is awesome and I expect cyborgism to be the frontier of the most useful and interesting cybermorph interaction and convergence with AI. We can already see parts of this in the software *Loom* that lets one explore all the possible responses one might get from a language model like ChatGPT as a tree of paths. 

![[Pasted image 20241014171721.png]]
*[The Loom](https://generative.ink/posts/loom-interface-to-the-multiverse/) - an interface into the multiverse of LLM completions*

Human-AI interfaces are still in their early and experimental stages and it will be especially interesting to see how we will interface with the *latent* knowledge of AI, such as with Loom. The current interfaces still play into the last decade's paradigms and feel much like having an assistant ([Claude with artifacts](https://support.anthropic.com/en/articles/9945615-intro-to-artifacts) and [other agents](https://x.com/bshlgrs/status/1840577720465645960)) or a pair programming buddy ([Copilot](https://github.com/features/copilot) and [Cursor](https://www.cursor.com/)).

Might we be able to augment our own brain by inserting live sidenotes into our text? Will future word processors show the trees of LLM completions as live paths to travel, all transmitted to us via voice in a multimodal interface while we type? Can we use [RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) to add references while we type and correct factual statements?

### Becoming cybermorph

If you've read all the way to here, I hope you find this idea exciting! If we begin seeing cognition and intelligence as a moving target, the technologies we develop take on a completely different light and we'll be able to aim for new heights of experience.

Because it looks to me like we won't just be able to solve problems with technology but that we'll actually be able to change how our consciousness and experience works in the first place!

Good luck out there, fellow cybermorph.

## Reading list
With this introduction, I don't want to leave you without anything to lead you further down this rabbit hole. The non-fiction:
- [Tools for Thought](https://numinous.productions/ttft/): This is an exploration of how we might develop tools for thought from Andy Matuschak and Michael Nielsen. You will find Andy on the internet and Michael in various podcasts talking about this concept as well.
- [The Extended Mind](https://academic.oup.com/analysis/article-abstract/58/1/7/153111?redirectedFrom=fulltext&login=false): Foundational reading. Important to understand the philosophical underpinnings, as defined by Andy Clark and David Chalmers.
- [The Brain from Inside Out](https://academic.oup.com/book/35081): A great book by the amazing neuroscientist György Buzsáki. Should be accessible to most tech-savvy or academically minded individuals but gives you the foundation to understand how we rewire our brains along with some of the history of brain sciences.
- [Hacker's Manifesto](https://phrack.org/issues/7/3.html): Written by a cringe-y teen in the 80s after his arrest for hacking the school network, this became a cult classic for what hackers were really about.
- Optionally, if you're interested in reading more about how AI might change our experience, check out [ReadingWhatWeCan](https://readingwhatwecan.com/), a reading list I made. The first text by Vernor Vinge is especially good and succinct.

And the fiction:
- [Neuromancer](https://www.goodreads.com/book/show/6088007-neuromancer): Probably one of the better depictions of what a cyberspace-enabled existence in a dystopian future looks like, this book combines untasteful culture with imaginings of what our interaction with cyberspace might be like.

There's plenty more where these come from but I'll leave it at that. A lot of cybermorph cultural heritage also extends from gender theory (abolishing static identities), accelerationism (the Nick Land type) and the early internet writings about what a decentralized web could really *be* for human identity.

> Like swarms of avatars subsisting on a star  
> Desiccant souls in searing light exposed  
> Moths to the raging inferno of the binary
> 
> *[Cybermorphism / Mainframe](https://youtu.be/1LQfI_H5nBg)*
